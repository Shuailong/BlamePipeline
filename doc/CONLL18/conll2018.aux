\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{alicke2000culpable,mary1992risk,farmer2006aids,gephart1993textual,hobolt2014blaming,hood2010blame,shaver2012attribution,tilly2009credit}
\citation{boin2010leadership,malhotra2008attributing}
\citation{nicolno,tourish2012metaphors}
\citation{olmeda2008reversal}
\citation{rule2015lexical,bail2016combining}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{figure:introdemo}{{1}{1}{Three sentences containing blame ties from our dataset. The red/{\bf bold} words are entities involved in a blame tie, and the blue/{\it italic} words are supporting evidence that the blame tie exists.\relax }{figure.caption.1}{}}
\citation{luttrell2013assessing}
\citation{o2010tweets}
\citation{o2010tweets}
\citation{bamman2015open}
\citation{bamman2015open}
\citation{preoctiuc2017beyond}
\citation{preoctiuc2017beyond}
\citation{rule2015lexical}
\citation{bail2016combining}
\citation{miwa2016end}
\citation{nadeau2007survey,rink2010utd}
\citation{zheng2017joint}
\citation{gentzkow2010drives,groseclose2005measure}
\newlabel{task}{{2}{2}{Related Work}{section.2}{}}
\newlabel{dataset}{{3}{2}{Dataset}{section.3}{}}
\newlabel{table:dataset}{{1}{3}{Dataset size for the three newspapers. USA: {\it USA Today}. NYT: {\it The New York Times}. WSJ: {\it The Wall Street Journal}.\relax }{table.caption.2}{}}
\newlabel{table:samplestats}{{2}{3}{Samples statistics.\relax }{table.caption.3}{}}
\newlabel{entitymodel}{{5.1}{3}{Entity Prior Model}{subsection.5.1}{}}
\newlabel{table:samples}{{3}{4}{An article titled {\it Rate Cut Has Foes on Main Street} ({\it The Wall Street Journal}, September 2007). Top: paragraphs of the article containing blame patterns. The blame entities are in {\bf bold face}. Bottom: blame ties extracted from the article.\relax }{table.caption.4}{}}
\newlabel{table:samplemat}{{4}{4}{Matrix representation of the blame ties in Table~\ref {table:samples}.\relax }{table.caption.5}{}}
\newlabel{table:lengthstatistic}{{5}{4}{Sentence and words statistics.\relax }{table.caption.6}{}}
\newlabel{contextmodel}{{5.2}{4}{Context Model}{subsection.5.2}{}}
\citation{manning-EtAl:2014:P14-5}
\citation{DBLP:journals/corr/abs-1301-3781}
\citation{pennington2014glove}
\citation{Peters:2018}
\newlabel{figure:contextmodel}{{2}{5}{Context Model. LSTM is used to encode the sentences, and the hidden vectors at the positions of the entity are pooled together into a single vector to represent the context of the entity. The source entity context vector and the target entity context vector are concatenated together to be sent to the prediction layer. The prediction layer predicts 1 if the source to target blame tie exists otherwise 0.\relax }{figure.caption.7}{}}
\newlabel{experiment}{{6}{5}{Experiments}{section.6}{}}
\citation{kingma2014adam}
\citation{hinton2012improving}
\citation{maaten2008visualizing}
\newlabel{table:BoEresult}{{6}{6}{Experiment results of Context Model using different pooling functions.\relax }{table.caption.8}{}}
\newlabel{table:pretrain}{{7}{6}{Experiment results of Context Model using different pretrained word vectors.\relax }{table.caption.9}{}}
\newlabel{table:finalresult}{{8}{6}{Experiment results of Entity Model, Context Model and Combined Model on KNOWN data and ALL data.\relax }{table.caption.10}{}}
\newlabel{table:comparison}{{9}{7}{Model comparison between Context Model (CTX) and Combined Model (CMB) on dev/test sets.\relax }{table.caption.11}{}}
\newlabel{fig:emb}{{3}{7}{Entity embedding visualizations (only a subset of the entities are shown for legibility). Wall Street and McCain and close. Obama, Congress and Bernanke also form a group.\relax }{figure.caption.12}{}}
\bibdata{conll2018}
\bibcite{alicke2000culpable}{{1}{2000}{{Alicke}}{{}}}
\bibcite{bail2016combining}{{2}{2016}{{Bail}}{{}}}
\newlabel{table:news}{{10}{8}{An article from {\it Bloomberg} published on January 18, 2018. Top: paragraphs of the article containing blame patterns. The blame entities are in {\bf bold face}. Bottom: major entities appearing in the article.\relax }{table.caption.13}{}}
\newlabel{table:newsresultcontext}{{11}{8}{Prediction result of the article in Table~\ref {table:news} using the Context Model. The number represents the probability that the blame ties exists between the two corresponding entities.\relax }{table.caption.14}{}}
\bibcite{bamman2015open}{{3}{2015}{{Bamman and Smith}}{{}}}
\bibcite{boin2010leadership}{{4}{2010}{{Boin et~al.}}{{Boin, Hart, McConnell, and Preston}}}
\bibcite{mary1992risk}{{5}{1992}{{Douglas}}{{}}}
\bibcite{farmer2006aids}{{6}{2006}{{Farmer}}{{}}}
\bibcite{gentzkow2010drives}{{7}{2010}{{Gentzkow and Shapiro}}{{}}}
\bibcite{gephart1993textual}{{8}{1993}{{Gephart}}{{}}}
\bibcite{groseclose2005measure}{{9}{2005}{{Groseclose and Milyo}}{{}}}
\bibcite{hinton2012improving}{{10}{2012}{{Hinton et~al.}}{{Hinton, Srivastava, Krizhevsky, Sutskever, and Salakhutdinov}}}
\bibcite{hobolt2014blaming}{{11}{2014}{{Hobolt and Tilley}}{{}}}
\bibcite{hood2010blame}{{12}{2010}{{Hood}}{{}}}
\bibcite{kingma2014adam}{{13}{2014}{{Kingma and Ba}}{{}}}
\bibcite{luttrell2013assessing}{{14}{2013}{{Luttrell et~al.}}{{Luttrell, Atkinson, Rosenblum et~al.}}}
\bibcite{maaten2008visualizing}{{15}{2008}{{Maaten and Hinton}}{{}}}
\bibcite{malhotra2008attributing}{{16}{2008}{{Malhotra and Kuo}}{{}}}
\bibcite{manning-EtAl:2014:P14-5}{{17}{2014}{{Manning et~al.}}{{Manning, Surdeanu, Bauer, Finkel, Bethard, and McClosky}}}
\bibcite{DBLP:journals/corr/abs-1301-3781}{{18}{2013}{{Mikolov et~al.}}{{Mikolov, Chen, Corrado, and Dean}}}
\bibcite{miwa2016end}{{19}{2016}{{Miwa and Bansal}}{{}}}
\bibcite{nadeau2007survey}{{20}{2007}{{Nadeau and Sekine}}{{}}}
\bibcite{nicolno}{{21}{2016}{{Nicol}}{{}}}
\bibcite{o2010tweets}{{22}{2010}{{O'Connor et~al.}}{{O'Connor, Balasubramanyan, Routledge, and Smith}}}
\bibcite{olmeda2008reversal}{{23}{2008}{{Olmeda}}{{}}}
\bibcite{pennington2014glove}{{24}{2014}{{Pennington et~al.}}{{Pennington, Socher, and Manning}}}
\bibcite{Peters:2018}{{25}{2018}{{Peters et~al.}}{{Peters, Neumann, Iyyer, Gardner, Clark, Lee, and Zettlemoyer}}}
\bibcite{preoctiuc2017beyond}{{26}{2017}{{Preo{\c {t}}iuc-Pietro et~al.}}{{Preo{\c {t}}iuc-Pietro, Liu, Hopkins, and Ungar}}}
\bibcite{rink2010utd}{{27}{2010}{{Rink and Harabagiu}}{{}}}
\bibcite{rule2015lexical}{{28}{2015}{{Rule et~al.}}{{Rule, Cointet, and Bearman}}}
\bibcite{shaver2012attribution}{{29}{2012}{{Shaver}}{{}}}
\bibcite{tilly2009credit}{{30}{2009}{{Tilly}}{{}}}
\bibcite{tourish2012metaphors}{{31}{2012}{{Tourish and Hargie}}{{}}}
\bibcite{zheng2017joint}{{32}{2017}{{Zheng et~al.}}{{Zheng, Wang, Bao, Hao, Zhou, and Xu}}}
\bibstyle{acl_natbib_nourl}
